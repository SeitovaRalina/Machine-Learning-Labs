{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression(): \n",
    "  \n",
    "    # Initiating the parameters. \n",
    "    def __init__(self, n_iter, learning_rate): \n",
    "        self.learning_rate = learning_rate \n",
    "        self.n_iter = n_iter \n",
    "  \n",
    "    def fit(self, X, Y): \n",
    "  \n",
    "        # No. of training examples and no. of features. \n",
    "        self.m, self.n = X.shape     # Number of rows and columns \n",
    "        # Initiating the weight and bias \n",
    "        self.w = np.zeros((self.n, 1)) \n",
    "        self.b = 0\n",
    "        self.X = X \n",
    "        self.Y = Y \n",
    "  \n",
    "        # Implementing the gradient descent. \n",
    "        for i in range(self.n_iter): \n",
    "            self.update_weigths() \n",
    "  \n",
    "    def update_weigths(self): \n",
    "        Y_prediction = self.predict(self.X) \n",
    "  \n",
    "        # Calculating gradients \n",
    "        dw = -(self.X.T).dot(self.Y - Y_prediction)/self.m \n",
    "  \n",
    "        db = -np.sum(self.Y - Y_prediction)/self.m \n",
    "  \n",
    "        # Updating weights \n",
    "        self.w = self.w - self.learning_rate * dw \n",
    "        self.b = self.b - self.learning_rate * db \n",
    "  \n",
    "    def predict(self, X): \n",
    "        return X.dot(self.w) + self.b \n",
    "  \n",
    "    def print_weights(self): \n",
    "        print('Weights for the respective features are :') \n",
    "        print(self.w) \n",
    "        print() \n",
    "  \n",
    "        print('Bias value for the regression is ', self.b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "        A simple class to perform a task of Linear Regression.\n",
    "        \n",
    "        Steps\n",
    "        -----\n",
    "        * Find the hypothesis using y = mX + c, where X is as input vector.\n",
    "        * Find the cost value.\n",
    "        * Use gradient descent to update each parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            This method doesnot take any initial attributes. You will tune the attributes later using methods.\n",
    "        \"\"\"\n",
    "        # instantiate train data and label\n",
    "        self.X = None \n",
    "        self.y = None\n",
    "        # to store all costs, initially cost is infinity\n",
    "        self.costs = [np.inf]\n",
    "        \n",
    "        # m is for len(X)\n",
    "        self.m = None\n",
    "        \n",
    "        # learning rate\n",
    "        self.alpha = None\n",
    "        \n",
    "        # our all parameters(from each iterations), m, c in y = mx + c\n",
    "        self.all_parameters = []\n",
    "    \n",
    "    def hypothesis(self, x):\n",
    "        \"\"\"\n",
    "            A method to perform linear operation(mx + c) and return.\n",
    "            \n",
    "            Formula:\n",
    "            --------\n",
    "            \\begin{equation}\n",
    "            h{_\\theta}{(x)} = {\\theta}^{T}x = \\theta{_0} + \\theta{_1} x_1 \n",
    "            \\end{equation}\n",
    "\n",
    "        \"\"\"\n",
    "        # y = XM, where X is of shape (M, N) and M of (N, 1)\n",
    "        h = np.dot(x, self.parameters)\n",
    "        return h\n",
    "    \n",
    "    def cost(self, yp, yt):\n",
    "        \"\"\"\n",
    "            yp: Predicted y.\n",
    "            yt: True y.\n",
    "\n",
    "            Formula:\n",
    "            -------\n",
    "            \\begin{equation}\n",
    "            J{(\\theta)} = \\frac{1} {2m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)})^2\n",
    "            \\end{equation}\n",
    "\n",
    "        \"\"\"\n",
    "        # find actual deviation\n",
    "        d = yp - yt \n",
    "        # constant 1/2m\n",
    "        c = 1/(2 * self.m)\n",
    "        # finally, sum the square of gradients \n",
    "        delta = c * np.sum(d ** 2)   \n",
    "        return delta\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        \"\"\"\n",
    "            A method to perform parameter update based on gradient descent algorithm.\n",
    "            \n",
    "            Rule:\n",
    "            -----\n",
    "            \\begin{equation}\n",
    "            \\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)}) x_j^{(i)}\n",
    "            \\end{equation}\n",
    "\n",
    "        \"\"\"\n",
    "        temp_theta = self.parameters\n",
    "        # for each theta\n",
    "        for j in range(len(temp_theta)):\n",
    "            grad = np.sum((self.hypothesis(self.X) - self.y)*np.array(self.X[:,j]).reshape(self.m, 1))\n",
    "            temp_theta[j] = self.parameters[j] - (self.alpha / self.m) * grad\n",
    "        self.parameters = temp_theta\n",
    "        self.all_parameters.append(temp_theta.flatten())\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            A method to return prediction. Perform preprocessing if model was trained preprocessed data.\n",
    "\n",
    "            Preprocessing:\n",
    "            -------------\n",
    "            X = (X - X.mean()) / X.std()\n",
    "        \"\"\"\n",
    "        if self.preprocessed != True:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            X = self.normalize(X)\n",
    "        return self.hypothesis(X)\n",
    "    \n",
    "    def visualize(self, thing=\"cost\"):\n",
    "        \"\"\"\n",
    "            Visualise the plots.\n",
    "            Available thing:\n",
    "            ----------\n",
    "            i. cost: Cost value vs iteration\n",
    "            ii. param: Parameters vs iteration\n",
    "            iii. all: cost and param vs iteration\n",
    "        \"\"\"\n",
    "        legend = [\"Loss\"]\n",
    "        if thing == \"cost\" or thing == \"all\":\n",
    "            plt.title(\"Loss vs step.\")\n",
    "            plt.grid(True)\n",
    "            plt.plot(self.costs)\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend(legend)\n",
    "    \n",
    "    #def visualize_parameters(self):\n",
    "        if thing == \"param\" or thing == \"all\":\n",
    "            plt.title(\"Parameter on each step.\")\n",
    "            plt.grid(True)\n",
    "            plt.plot(self.all_parameters)\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Parameters\")\n",
    "            l = [fr\"$\\Theta{i}$\" for i in range(len(self.parameters))]\n",
    "            if thing == \"all\":\n",
    "                l.insert(0, legend[0])\n",
    "                legend = l\n",
    "            else:\n",
    "                legend = l\n",
    "            plt.legend(legend)\n",
    "        \n",
    "    def normalize(self, X):\n",
    "        \"\"\"\n",
    "            X: Input training data.\n",
    "            Returns: normalized x.\n",
    "            \n",
    "            Normalization:\n",
    "            --------------\n",
    "            X = (X - X.mean()) / X.std()\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        means, stds = [], []\n",
    "        normalized_x = X.copy()\n",
    "        # since we need to work with only the column we will iterate over shape[1]\n",
    "        for col in range(normalized_x.shape[1]):\n",
    "            means.append(normalized_x[:, col].mean())\n",
    "            stds.append(normalized_x[:, col].std())\n",
    "            if not col: continue\n",
    "            normalized_x[:, col] = (normalized_x[:, col] - means[-1])/stds[-1]\n",
    "        # store the means and stds. We need them on future.\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "        return normalized_x\n",
    "\n",
    "    def fit(self, X, y, error_threshold = 0.001, preprocessed = True, alpha=0.01, show_every=100, iterations=1500):\n",
    "        \"\"\"\n",
    "            X: input train (m X n),  if is not normalized and added axis for bias, use preprocessed=False.\n",
    "            y: train label (n X 1)\n",
    "            error_threshold: How much error is tolerable?\n",
    "            preprocessed: does train data has added axis for bias and normalized?\n",
    "            alpha: learning rate(update step)\n",
    "            show_every: how often to show cost?\n",
    "            iterations: how many steps to run weight update(gradient descent)?\n",
    "        \"\"\"\n",
    "        \n",
    "        self.preprocessed = preprocessed\n",
    "        # if data already have bias added and normalized, leave it.\n",
    "        if preprocessed!=True:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            X = self.normalize(X)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # how many of training examples are there?\n",
    "        self.m = len(X)\n",
    "        \n",
    "        # how many of parameters?\n",
    "        # initialize it to 0, shape must be (num. features + 1, 1)\n",
    "        # X here is normalized i.e it already have axis for bias\n",
    "        self.parameters = np.zeros((X.shape[1], 1))\n",
    "        costs = [np.inf] #Used to plot cost as function of iteration\n",
    "        i = 0\n",
    "        \n",
    "        # if our update is not done for iterations and error is pretty high, \n",
    "        while (iterations>i and costs[-1]>error_threshold):\n",
    "            # find the cost value\n",
    "            cost_value = self.cost(self.hypothesis(self.X), self.y)\n",
    "            costs.append(cost_value)\n",
    "            # perform gradient descent and update param\n",
    "            self.gradient_descent() \n",
    "            if i % show_every == 0:\n",
    "                print(f\"Step: {i} Cost: {round(cost_value, 4)}.\")\n",
    "            \n",
    "            i+=1\n",
    "        self.costs = costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(normalized_x[:, :], y, iterations=1500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
